\documentclass[11pt]{article}
\usepackage{acl2012}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{url}
\DeclareMathOperator*{\argmax}{arg\,max}
\setlength\titlebox{6.5cm}    % Expanding the titlebox
\newcommand{\comment}[1]{}


\title{Geolocating Wikipedia Articles Using Label Propagation}

\author{
  Aidan Coyne \\
    Department of Computer Science\\
    The University of Texas at Austin\\
  {\tt coynea90@gmail.com} \\ 
  \And
  Prateek Maheshwari\\
    Department of Computer Science\\
    The University of Texas at Austin\\
  {\tt prateekm@utexas.edu}
}

\date{}

\begin{document}
\maketitle



\section{Introduction}
\comment{You can now enclose text in a comment block to comment it out inline}
Geographic information is relevant in many different contexts, for example, geographic information retrieval for exploring document collections \cite{}, 
toponym resolution in historical texts \cite{perseus project},  summarizing travelogues and travel recommendation \cite{hao},
socio-linguistic studies \cite{eisenstien}, targeted advertising on the internet etc. With the widespread use of mobile devices, geographic information is becoming increasingly 
ubiquitous and important. In this project we propose a method for predicting the geographic location of wikipedia articles 
with a graph based semi-supervised algorithm and a small amount of labeled data.

Prior work on geolocating Wikipedia articles (\cite{wing-baldridge:11} \cite{rolleretal:12}) has so far only used 
a subset of the English language Wikipedia for training; specifically, it uses those articles that already have geolocation tags associated with them.  
These articles constitute a relatively small portion of the english language wikipedia. \comment{find actual figure}
This isn't surprising, since most articles on wikipedia are not about geographic locations, or objects at specific locations.
On the other hand, the number of articles that have links to or from such geolocated articles is much larger, 
which can be a potential source of useful information. \comment{ find figure  of the wikipedia corpus}

Within the geotagged articles, earlier approaches use the article content indirectly by building unigram language models from the text of the articles during training,
and then comparing similarity of test documents with these language models. This ignores the potentially useful information present in the form of hyperlinks between articles,
which is a strong indicator of their semantic and geographic relatedness, and hence can be used to improve prediction accuracy.

Another assumption inherent in prior work is that each article is associated with a single location, which is taken to be the location that the article is geo-tagged with. 
While this is a fair assumption for some articles, e.g., those about historical monuments and physical objects, it's not so obvious that the assumption also holds for articles about 
geographically distributed concepts such as states (e.g., Texas) or personal biographies (e.g., George Washington). 

Our hypothesis is that we can infer the location for non-geotagged articles by using the location information present in other geotagged articles that they link to, or receive incoming links from. 
Unlike prior work, our approach makes use of the link structure of the articles in addition to the location metadata and text based language models. 
We use the label propagation algorithm \cite{} to infer the location of non geotagged articles. We also lift the single location restriction by allowing each article 
to have a distribution over all locations, and including the articles in the language models for multiple locations that are associated with it.

The major motivation and hypothesis behind our project is that these additional geo-located articles can provide additional information for improving the language models based approach 
used in prior work by appropriately smoothing the language models.



\section{Related Work}
\subsection{Geolocation}
%something on using gazetteers for toponym resolution
Geolocation has traditionally been done \comment{improve this part} using information from gazetteers \cite{} or as an IR task, as explained in \cite{skiles:12}. 
Most IR-based approaches divide the world into discrete grid cells, each of which are treated as a pseudo-documents that consist of texts from documents located in them.
Documents are then geolocated by comparing their language model with the language models of each of the grid cells and assigning the document to the cell with which 
they have the highest similarity scores. \cite{eisenstein sociaologaige} exemplify this language modeling approach by explicitly removing geo-references from documents 
and predicting location using only the linguistic and dialectical features present in the document text.

The use of wikipedia for article geolocation, classification and toponym resolution was suggested in \cite{Overall09}. 
While he only uses the metadata associated with the articles, others have extended his approach by adopting the language modeling approach and 
using the article text to predict article location.This is the approach used by \cite{wing-baldridge:11}, who divide the world into a uniform grid of equal cell size in degrees.
\cite{rolleretal:12} develop an adaptive grid based on KD-trees that splits cells such that they contain roughly equal number of documents each.
A different model for geolocation is presented by \cite{eisensteinetal:11}; who use a general generative model as an alternative to LDA, and model distributions 
as a mixture of gaussians over the earth's surface instead of discrete grid cells. 

Furthermore, (kumar2011supervised) have shown that language models learnt from wikipedia can be used for prediction tasks in other domains. This is an 
promising result that suggests that the language models learnt from wikipedia may be used for geolocating other documents. This is however beyond the scope
of this project.


\subsection{Label Propagation}
Label propagation is a general purpose graph-based semi-supervised learning algorithm \cite{talukdar:09} \cite{talukdar:10}. 
Given a graph G = {V, E, W}, where V are the graph vertices, E are the edges and W is a matrix representing the edge weights, 
the label propagation algorithm produces a set of class labels for each node in the graph starting from a small seed set of labeled nodes. 
The algorithm iteratively propagates the labels on the seed nodes to their neighbors in proportion to the weight of the edge between them. 
The edge weights are chosen to be proportional to the similarity between the nodes. 
Label propagation has been successfully used for sentiment analysis \cite{sperisou}, community detection \cite{talukdar}, 
classification and ranking \cite{something} \comment{and recommendation, youtube}. 




\section{Plan of Work}
\subsection{Dataset and Tools}
Following the methodology in \cite{wing-baldridge:11}, we will use the 2010 wikipedia dump for evaluation. 
After cleanup and processing, the dump contains 3,431,722  articles, of which 488,269 are geotagged. 
The number of articles that link directly to geotagged articles is to be determined.

We will use the textgrounder \cite{textgrounder} toolkit for creating the grids to be used for document geolocation and for evaluating 
and comparing our results with prior work.
 
We will use the Junto toolkit \cite{junto} for label propagation using the Modified Adsorption algorithm.

\subsection{Issues to Explore}
In the first stage of the project, we will determine the accuracy of label propagation for geolocating wikipedia documents. 
For this, we will train and test on geotagged wikipedia documents, and compare the performance of label propogation with prior work that uses language models.
If label propogation give promising results, we can examine the possibility of augmenting the pseudo-documents in language-models based approaches 
with the additional info from the new documents geolocated using label propogation.

We will initially evaluate and tune the performance of label propagation using a uniform grid, since it is easier to set up and use. 
In a later stage, we can easily substitute it with the more accurate KD-trees based adaptive grid of \cite{rolleretal:12}.

The nodes in the graph to be used for propagating labels will be wikipedia documents, and the labels will be the grid cells that the documents fall in.
We will use a fixed subset of the geotagged portion of wikipedia as the seed nodes. It would be instructive to examine the accuracy of label propagation
as a function of the number of seed documents used. 

There are some other considerations in determining the structure of the graph. The first question is whether the graph should be directed, 
with edges pointing from referring document to the referred document, or undirected. The second set of questions pertain to the method to be used
for inducing the edges between documents; for example, whether the edges are based solely on hyperlinks between nodes, whether the edges are transitive, 
and whether we should induce a dense graph with many edges between nodes, or a sparse graph with a few strong connections. Additionally, we could induce 
an edge between all documents that fall in a particular grid cell to increase locality, or between documents in adjacent cells, to introduce smoothing.

The third issue is to determine the weights of edges. Intuitively, the weight of an edge should be proportional to the similarity of the two nodes (documents). 
This notion of similarity can be formalized in a number of ways. One approach would be to take the similarity between the language models for the 
two documents as the edge weights. This could be the cosine similarity, tf-idf weighted cosine similarity or the KL divergence.
An straightforward alternative to begin with would be to use the same edge weight for all edges. 
 
Furthermore, since each document can potentially be related to more than one location, we will examine whether it is better to clamp the label distributions 
on the seed documents to the single labels from their geotags, or to allow them to change and acquire nonzero weights for other locations.
One reason to do that would be to allow a document label to be influenced by closely tied geographic locations. For example, the distribution for the article for 
Lake Austin would have a nonzero weight for the location for Travis County, due their mutual references to the article for Austin. As hypothesized earlier, this
might be useful in geolocating concepts that span multiple locations.

After further exploration of the datasets and tools, we will systematically test these alternatives, and determine the best models and parameters to use 
for the label propagation algorithm.


\section{Evaluation}
We will test the performance of label propagation against the geolocation results from textgrounder. 
It should be noted that the aim of the project is not to beat the results obtained using textgrounder,
but instead to get a reasonable geolocation accuracy on non-geotagged articles 
so that they can be included in the language model for textgrounder cells. 
If we can obtain an accuracy close to that of textgrounder on the geotagged corpus, 
we can be sure that the results for non-geotagged documents will be useful.

To evaluate the performance of the models, we will use measures based on error distances, e.g, mean and median error between predicted and actual distance,  
instead of IR based methods like precision or recall. \cite{Roller et al} show that using the centroid of locations of documents in the grid cell as the cell's location 
results in more accurate predictions than using the mean of document locations (as in \cite{wing-baldridge:11}), even in the uniform-grid case. Therefore, we will
use this approach in our evaluation.


\section{Milestones}
1. Dataset preparation and tools exploration
2. Establishing the hypotheses to be tested
3. Comparing the alternatives and determining the best model.
4. Measuring accuracy of label propagation on geotagged documents and comparing with language model approaches.
5. [Optional] 

\section{Future Work}

Although beyond the scope of this project, once we predict the location of the unlabeled documents, we can include the text from those documents
while constructing the language models for the cells they belong to provide additional information or smoothing. For this, an important problem is to 
determine this set of documents that can benefit from label propagation and should be included in the final language model.

Some other potentially related, but time consuming, alternatives to test would be:
Using mean-shift clustering as in \cite{Grauman} for detecting locations instead of a discrete grid.
Using NER to detect location references in addition to explicit links.
Using label propagation for geolocating twitter users from their connection graph.

%\par
%The motivation for all of this is to extend geolocation to the vast unlabeled
%section of wikipedia; with two different models, each with a variety of
%parameters, there are multiple ways to go about this.
%One scheme would be to use the geolocated portion to train Textgrounder and
%compare its predictions for the unlabeled set to the predictions yielded by
%running label propagation over the combined sets.

\bibliographystyle{acl2012}
\bibliography{refs}
\end{document}
